# **MagNET Thesis**

ğŸ“¡ **A Thesis Project Based on the MagNET Database**

This repository contains my work on applying machine learning techniques to analyze and model data from the **MagNET** database. The project focuses on **predicting magnetic core loss using wavelet-based transformations and deep learning models**, with an additional **ablation study** to evaluate the impact of scalograms.

ğŸš§ **This repository is still a work in progress.** Stay tuned for updates!

---

## **ğŸ”¹ How to Run the Project**

### **1ï¸âƒ£ Single Run (Scalogram Model)**
To train and evaluate the **scalogram-based model** with specific hyperparameters, run:

```bash
python3 wavelet_main.py --num_epoch 50 --learning_rate 0.001 --batch_size 128
```

### **2ï¸âƒ£ Single Run (Ablation Model)**
To train and evaluate the **ablation model** (using raw voltage data, no scalograms) with specific hyperparameters, run:

```bash
python3 ablation_main.py --num_epoch 50 --learning_rate 0.001 --batch_size 128
```
### **3ï¸âƒ£ Single Run (Transfer Learning Model)**

To train and evaluate the transfer learning model (based on ResNet18 with 224x224 scalograms) with specific hyperparameters, run:
```bash
python3 transfer_main.py --num_epoch 50 --learning_rate 0.001 --batch_size 128
```
    Use --use_pretrained to load pre-trained ResNet18 weights (default: True).
    Use --freeze_layers to freeze all layers except the final fully connected layer (default: False).

### **4ï¸âƒ£ Multiple Runs (Hyperparameter Sweep)**

To perform multiple runs with different hyperparameter values:

â€¢ Scalogram Model:
```bash
python3 manual_sweep.py
```
â€¢ Ablation Model:
```bash
python3 ablation_manual_sweep.py
```
â€¢ Transfer Learning Model:
```bash
python3 transfer_manual_sweep.py
```
These scripts execute experiments with preset hyperparameter combinations and log the results for evaluation.

### **5ï¸âƒ£ Optimization Run**

To run hyperparameter optimization using Optuna:

â€¢ Scalogram Model:
```bash
python3 wavelet_main.py --optimize
```
â€¢ Ablation Model:
```bash
python3 ablation_main.py --optimize
```
â€¢ Transfer Learning Model:
```bash
python3 transfer_main.py --optimize
```

---

## **ğŸ”¹ Repository Structure**

ğŸ“ **/data/** â†’ Processed and raw datasets (not included in the repo) and dataset-related scripts  
ğŸ“ **/models/** â†’ Contains model architecture definitions  
ğŸ“ **/checking_scripts/** â†’ Directory containing helper scripts for dataset preprocessing, visualization, and consistency checks  
ğŸ“ **/utils/** â†’ Model training scripts  

### **Main Files**
- `wavelet_main.py` â†’ **Main script for training the scalogram-based model**
- `ablation_main.py` â†’ **Main script for training the ablation model using raw voltage data (no scalograms)**
- `transfer_main.py` â†’ **Main script for training the transfer learning model (ResNet18-based)**
- `manual_sweep.py` â†’ **Performs multiple runs for hyperparameter tuning of the scalogram model**
- `ablation_manual_sweep.py` â†’ **Performs multiple runs for hyperparameter tuning of the ablation model**
- `transfer_manual_sweep.py` â†’ **Performs multiple runs for hyperparameter tuning for the transfer learning model**
### **Data Processing Files**
- `data/wavelet_coreloss_dataset.py` â†’ **Processes and transforms data into scalograms for the scalogram model**
- `data/voltage_coreloss_dataset.py` â†’ **Processes raw voltage data for the ablation model**
- `data/wavelet_coreloss_dataset_224.py` â†’ **Processes and transforms data into 224x224 scalograms for the transfer learning model**
- `data/data_processing.py` â†’ **Splits dataset into train/validation/test, checks core loss distribution, and creates DataLoaders (used by scalogram model)**
- `data/dataloaders_ablation.py` â†’ **Splits dataset and creates DataLoaders specifically for the ablation model**
### **Model Architecture Files**
- `models/wavelet_model.py` â†’ **Neural network architecture for the scalogram-based model**
- `models/ablation_model.py` â†’ **Neural network architecture for the ablation model (no scalograms)**
- `models/transfer_model.py` â†’ **Neural network architecture for the transfer learning model (ResNet18-based)**
### **Training Scripts**
- `utils/train_utils.py` â†’ **Training and evaluation utilities shared across models**
### **Checking Scripts**
- `checking_scripts/coreloss_verification.py` â†’ **Validates core loss calculations**
- `checking_scripts/model_consistency_check.py` â†’ **Ensures model weight consistency across runs**
- `checking_scripts/inspect_dataset.py` â†’ **Explores and analyzes the dataset**
- `checking_scripts/sanity_check.py` â†’ **Checks model predictions on the test set, compares actual vs. predicted values, and provides statistics (e.g., RÂ², MAE, MSE)**
- `checking_scripts/trapcheck.py` â†’ **Compares preprocessed and original datasets, checks waveform consistency**
- `checking_scripts/check_scalograms.py` â†’ **Visualizes and verifies original and resized scalograms for the transfer learning model**
---

## **ğŸ”¹ Requirements**

Ensure you have all dependencies installed before running the scripts:

```bash
pip install -r requirements.txt
```

If `requirements.txt` is not available, install the key dependencies manually:

```bash
pip install numpy torch torchvision matplotlib pandas tqdm optuna
```

---

## **ğŸ”¹ Current Progress**

âœ”ï¸ Implemented wavelet transformations for input data  
âœ”ï¸ Created dataset classes for efficient loading (scalogram and raw voltage)  
âœ”ï¸ Built deep learning models for core loss prediction (scalogram and ablation)  
âœ”ï¸ Added Weights & Biases (wandb) logging for experiment tracking  
âœ”ï¸ Implemented ablation study to evaluate the impact of scalograms  
âœ”ï¸ Integrated transfer learning with a pre-trained ResNet18 model and 224x224 scalograms
ğŸš§ **Next Steps:** Optimize all models and finalize hyperparameter tuning  

---

## **ğŸ”¹ Contributing**

ğŸ‘¨â€ğŸ’» Contributions, suggestions, and feedback are welcome! Feel free to open an issue or submit a pull request.

---

## **ğŸ”¹ License**

ğŸ“œ This project is for academic and research purposes. Please cite appropriately if you use any part of it in your work.

---

## **ğŸ”¹ Acknowledgments**

Special thanks to the **MagNET database** for providing the data used in this thesis.

